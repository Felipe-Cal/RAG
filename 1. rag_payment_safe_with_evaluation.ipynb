{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccfb0b4",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Example for Payment Systems\n",
    "\n",
    "This notebook demonstrates a **production-oriented RAG pipeline** designed for **payment and risk-related use cases**.\n",
    "\n",
    "The focus is on:\n",
    "- Grounded generation using internal knowledge\n",
    "- Safety and explainability\n",
    "- Explicit **evaluation and observability hooks**\n",
    "\n",
    "The architecture shown here is suitable for regulated environments where AI assists humans rather than making decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8829e",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "This RAG system has two phases:\n",
    "\n",
    "1. **Offline (batch) phase**\n",
    "   - Load internal payment and risk documents from the `data` folder.\n",
    "   - Split them into chunks, embed them, and build a vector index.\n",
    "   - This runs periodically whenever policies or documentation change.\n",
    "\n",
    "2. **Online (request-time) phase**\n",
    "   - Receive a question from a payments analyst (for example, why a transaction was high risk).\n",
    "   - Retrieve the most relevant chunks from the index.\n",
    "   - Ask the LLM to answer **using only that retrieved context**.\n",
    "   - Run validation rules and log inputs/outputs for audit and monitoring.\n",
    "\n",
    "The rest of the notebook walks through these two phases step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa73df",
   "metadata": {},
   "source": [
    "## 1. Technology Stack\n",
    "\n",
    "- **Python** for orchestration\n",
    "- **LlamaIndex** for document ingestion, chunking, and retrieval\n",
    "- **Vector-based RAG** for grounding responses in internal knowledge\n",
    "- **OpenAI-compatible LLM interface** (swappable with other providers)\n",
    "\n",
    "This notebook focuses on architectural clarity rather than framework-specific optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081e234",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "In a production environment, these dependencies would be baked into the runtime image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a18a9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2747484933.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install llama-index chromadb llama-index-llms-ollama llama-index-embeddings-huggingface\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index chromadb llama-index-llms-ollama llama-index-embeddings-huggingface\n",
    "#pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d16f1",
   "metadata": {},
   "source": [
    "## 3. Imports and Environment Configuration\n",
    "\n",
    "In this step we:\n",
    "- Import the core building blocks from LlamaIndex.\n",
    "- Configure the **LLM** using Ollama (runs locally, no API key needed).\n",
    "- Configure the **embedding model** using HuggingFace (runs locally, no API key needed).\n",
    "\n",
    "**Prerequisites**: You need Ollama installed and running with a model downloaded.\n",
    "Run this in your terminal first:\n",
    "```bash\n",
    "# Install Ollama from https://ollama.ai\n",
    "ollama pull llama3.1:8b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a82a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba69d1",
   "metadata": {},
   "source": [
    "## 4. Offline Phase: Knowledge Ingestion\n",
    "\n",
    "The `./data` directory is assumed to contain internal payment-related documents such as:\n",
    "- Risk rules documentation\n",
    "- Compliance policies\n",
    "- Country-specific payment regulations\n",
    "\n",
    "This ingestion step runs offline and is re-executed whenever knowledge changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226c729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:36:12,447 - WARNING - incorrect startxref pointer(2)\n",
      "2025-12-17 15:36:12,448 - WARNING - parsing for Object Streams\n",
      "2025-12-17 15:36:12,452 - WARNING - incorrect startxref pointer(2)\n",
      "2025-12-17 15:36:12,453 - WARNING - parsing for Object Streams\n",
      "2025-12-17 15:36:12,457 - WARNING - incorrect startxref pointer(2)\n",
      "2025-12-17 15:36:12,457 - WARNING - parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625a73b",
   "metadata": {},
   "source": [
    "## 5. Build the Vector Index\n",
    "\n",
    "This step performs:\n",
    "- **Chunking**: splitting documents into smaller pieces\n",
    "- **Embedding**: converting text chunks into vector representations (using HuggingFace locally)\n",
    "- **Vector indexing**: storing embeddings for fast similarity search\n",
    "\n",
    "We also configure the **LLM** (Ollama with Llama 3.1 8B, running locally) that will be used to generate answers.\n",
    "\n",
    "The resulting index is the foundation of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d399349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:36:16,165 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-12-17 15:36:19,147 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1:8b\", request_timeout=120.0)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016f759",
   "metadata": {},
   "source": [
    "## 6. Online Phase: Query Execution\n",
    "\n",
    "In the online phase we simulate a **payments analyst** asking a question.\n",
    "The system will:\n",
    "- Take the analyst question as input.\n",
    "- Retrieve the most relevant chunks from the index.\n",
    "- Ask the LLM to answer using only that retrieved context.\n",
    "\n",
    "Example use case:\n",
    "**Explain why a payment transaction was flagged as high risk**.\n",
    "\n",
    "The goal is to help the analyst understand *possible reasons* based on internal policies and risk rules, not to automatically approve or reject the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f0772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:36:21,877 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "2025-12-17 15:36:26,849 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The procure-to-pay process involves several key steps. First, purchases over $500 require a formal purchase requisition, which must include certain details such as item description, quantity, estimated cost, vendor information, and business justification. The requisition must be submitted through the SAP procurement system.\n",
      "\n",
      "Once approved, a Purchase Order will be generated automatically. The PO must be sent to the vendor within 2 business days. Goods receipt must be recorded in SAP within 24 hours of delivery, and any discrepancies must be reported to Procurement immediately.\n",
      "\n",
      "Invoices must reference a valid Purchase Order number and undergo three-way matching with the PO and Goods Receipt records. Invoices with variances over 5% require manual review. Payments should be processed within 5 business days, and payments over $50,000 require dual authorization.\n",
      "\n",
      "To ensure compliance, all procurement activities are subject to internal audit, and documentation must be retained for 7 years.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "\n",
    "query = \"\"\"\n",
    "You are assisting a payments analyst.\n",
    "\n",
    "Question: What are the key steps in the procure-to-pay process and what controls should be in place?\n",
    "Explain using only the retrieved internal policies and guidelines.\n",
    "Do not make any approval or rejection decisions.\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "response = query_engine.query(query)\n",
    "query_latency = time.time() - start_time\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72de71",
   "metadata": {},
   "source": [
    "## 7. Retrieval Observability Hook\n",
    "\n",
    "Inspecting retrieved context is critical for debugging RAG quality.\n",
    "This allows operators to verify that the system is grounding responses in the correct documents.\n",
    "\n",
    "Here we inspect, for each retrieved chunk:\n",
    "- The similarity score (how close it is to the query in embedding space).\n",
    "- The source document metadata (for example, file name and page number).\n",
    "- A short snippet of the retrieved text.\n",
    "\n",
    "This is what lets a human analyst quickly see **where the model is getting its answer from**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "568ebde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Score: 0.7480235902677769\n",
      "Source: procure-to-pay-guideline.pdf, page 1\n",
      "Procure-to-Pay Guidelines\n",
      "Acme Corporation - Internal Policy Document\n",
      "Version 2.1 - Effective Date: January 2024\n",
      "1. Introduction\n",
      "This document outlines the procure-to-pay process for Acme Corporation.\n",
      "All employees must follow these guidelines when purchasing goods or services.\n",
      "2. Purchase Requisiti\n",
      "---\n",
      "Score: 0.6988250829971236\n",
      "Source: procure-to-pay-guideline.pdf, page 2\n",
      "5. Purchase Order Creation\n",
      "5.1 Once approved, a Purchase Order will be generated automatically.\n",
      "5.2 The PO must be sent to the vendor within 2 business days.\n",
      "5.3 PO numbers follow the format: PO-YYYY-XXXXXX\n",
      "6. Goods Receipt\n",
      "6.1 All goods must be inspected upon delivery.\n",
      "6.2 The receiving department \n",
      "---\n",
      "Score: 0.6985231131393258\n",
      "Source: procure-to-pay-guideline.pdf, page 3\n",
      "9. Emergency Purchases\n",
      "9.1 Emergency purchases may bypass normal approval workflow.\n",
      "9.2 Retroactive approval must be obtained within 48 hours.\n",
      "9.3 Emergency purchases are limited to $10,000 maximum.\n",
      "10. Compliance and Audit\n",
      "10.1 All procurement activities are subject to internal audit.\n",
      "10.2 Document\n"
     ]
    }
   ],
   "source": [
    "for source in response.source_nodes:\n",
    "    print(\"---\")\n",
    "    print(f\"Score: {source.score}\")\n",
    "    print(f\"Source: {source.node.metadata.get('file_name')}, page {source.node.metadata.get('page_label')}\")\n",
    "    print(source.node.text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051a7a6",
   "metadata": {},
   "source": [
    "## 8. Generation Observability Hook\n",
    "\n",
    "With local models (Ollama), there's no API cost, but we still want to monitor:\n",
    "- **Latency**: how long the query took end-to-end (retrieval + generation)\n",
    "- **Response length**: size of the generated answer\n",
    "- **Memory usage**: RAM consumed by the Python process\n",
    "- **CPU usage**: processor utilization\n",
    "\n",
    "These metrics help detect performance issues and resource constraints in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c1b810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query latency: 4.97 seconds\n",
      "Response length: 999 characters\n",
      "Process memory usage: 276.3 MB\n",
      "Process CPU usage: 1.0%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "print(f\"Query latency: {query_latency:.2f} seconds\")\n",
    "print(f\"Response length: {len(str(response))} characters\")\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "cpu_percent = process.cpu_percent(interval=0.1)\n",
    "\n",
    "print(f\"Process memory usage: {memory_mb:.1f} MB\")\n",
    "print(f\"Process CPU usage: {cpu_percent:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f68a5b",
   "metadata": {},
   "source": [
    "## 9. Output Validation (Structural Guardrail)\n",
    "\n",
    "In production, AI output should never be trusted blindly.\n",
    "Typical validation steps include:\n",
    "- Enforcing structured output (JSON / schemas)\n",
    "- Confidence thresholds\n",
    "- Consistency checks against deterministic systems\n",
    "\n",
    "Below is a simplified example of a post-generation validation hook.\n",
    "Here we specifically enforce a **\"no autonomous decisions\"** rule: the model is not allowed to say it will approve, reject, or block a payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca7db1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_valid': False, 'reasons': ['Contains forbidden term: approve']}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class ValidationResult(TypedDict):\n",
    "    is_valid: bool\n",
    "    reasons: list[str]\n",
    "\n",
    "\n",
    "def validate_response(text: str) -> ValidationResult:\n",
    "    lowered = text.lower()\n",
    "    forbidden_phrases = [\"approve\", \"reject\", \"block the payment\"]\n",
    "    hits = [p for p in forbidden_phrases if p in lowered]\n",
    "    return {\"is_valid\": len(hits) == 0, \"reasons\": [f\"Contains forbidden term: {h}\" for h in hits]}\n",
    "\n",
    "\n",
    "validation = validate_response(str(response))\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624019c6",
   "metadata": {},
   "source": [
    "## 10. Retrieval Quality Evaluation (Offline)\n",
    "\n",
    "Retrieval quality is evaluated separately from generation.\n",
    "\n",
    "Typical offline metrics:\n",
    "- Recall@K (did we retrieve the correct knowledge?)\n",
    "- Precision@K (how much noise was retrieved?)\n",
    "\n",
    "In practice, this requires a labeled dataset of questions and expected source documents.\n",
    "\n",
    "Below we create a **tiny synthetic example** with one question and its expected source document, just to illustrate how such an evaluation loop works in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55ba6afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:36:49,112 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@3: 1.0\n"
     ]
    }
   ],
   "source": [
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"What are the steps in the procure-to-pay process?\",\n",
    "        \"expected_doc_ids\": [\"procure-to-pay-guideline.pdf\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def recall_at_k(engine, eval_data, k: int = 3) -> float:\n",
    "    hits = 0\n",
    "    total = len(eval_data)\n",
    "\n",
    "    for row in eval_data:\n",
    "        res = engine.query(row[\"query\"])\n",
    "        retrieved_ids = [s.node.metadata.get(\"file_name\") for s in res.source_nodes[:k]]\n",
    "        if any(doc_id in retrieved_ids for doc_id in row[\"expected_doc_ids\"]):\n",
    "            hits += 1\n",
    "\n",
    "    return hits / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"Recall@3:\", recall_at_k(query_engine, eval_queries, k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f688d8d",
   "metadata": {},
   "source": [
    "## 11. End-to-End Evaluation Signals\n",
    "\n",
    "Beyond technical metrics, system success is measured by:\n",
    "- Analyst resolution time\n",
    "- Reduction in support escalations\n",
    "- Human corrections of AI output\n",
    "\n",
    "These signals feed back into improving retrieval, chunking, and prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b546be",
   "metadata": {},
   "source": [
    "## 12. Auditability and Governance\n",
    "\n",
    "For each request, a production system should store:\n",
    "- Input query and metadata\n",
    "- Retrieved document identifiers\n",
    "- Prompt version\n",
    "- Model version\n",
    "- Output and validation result\n",
    "\n",
    "This enables regulatory audits and incident investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201054f",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook demonstrates a **payment-safe RAG architecture** where:\n",
    "- Retrieval grounds model responses in internal knowledge\n",
    "- Generation is constrained and observable\n",
    "- Evaluation and validation are first-class concerns\n",
    "\n",
    "The same pattern can be extended with human-in-the-loop review, multi-model routing, and stricter compliance controls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f083c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18129821",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
